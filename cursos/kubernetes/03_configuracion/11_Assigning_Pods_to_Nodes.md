# Asignaci칩n de Pods a Nodos 游
*nodeSelector, nodeAffinity e nodeName*

游닄 [Documentaci칩n oficial](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/) 

![](https://vergaracarmona.es/wp-content/uploads/2022/10/Historia-de-los-contenedores.png)

En Kubernetes, os pods progr치manse nos nodos mediante *kube-scheduler*. O *kube-scheduler* 칠 un proceso de *control-plane* que **asigna os pods 칩s nodos**. Determina qu칠 nodos son lugares v치lidos para cada pod segundo as restricci칩ns establecidas e os recursos dispo침ibles. Nembargantes, pode haber casos nos que te침amos un pod que requira da GPU para funcionar segundo as nosas expectativas. Neste suposto, podemos necesitar controlar a programaci칩n do pod para asignalo a un nodo que te침a GPU dispo침ible.

Hai varias formas de controlar a programaci칩n (*kube-scheduler*) e t칩dolos enfoques recomendados utilizan *labels* para facilitar a selecci칩n.

## 九덢잺 Labels nos nodos

Antes de seguir avanzando, vexamos as sintaxis que nos servir치n para manexar *labels* nos nodos:

Engadir label a un nodo:

``` shell
# Sintaxis
kubectl label nodes <NODE-NAME> <KEY>=<VALUE>

# Exemplo:
kubectl label nodes node01 gpu=yes

```

Listar os nodos coa columna de t칩dalas labels separadas por comas. Podemos concretar o nodo que queremos ver se po침emos seu nome:

``` shell
# Sintaxis
kubectl get nodes <NODE-NAME> --show-labels

# Exemplos:
kubectl get nodes --show-labels

kubectl get nodes nodo-01 --show-labels
```

Borrar unha label:

``` shell
# Sintaxis
kubectl label node <NODE-NAME> <LABEL>-

# Exemplo:
kubectl label node node01  gpu-
```

## 九덢잺 nodeSelector

`nodeSelector` 칠 a forma m치is sinxela de asignar pods 칩s nodos. Na `spec` do Pod podemos engadir o campo `nodeSelector` coas *labels* que ten noso nodo destino. Kubernetes s칩 programar치 o Pod nos nodos que te침en as mesmas *labels*. 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector: 
    gpu: "yes" # Label pra seleccionar o nodo
```

丘멆잺 Se non hai nodos dispo침ibles coa *label* especificada o Pod quedar치 en estado pendente.

### 游뚾 Proba nodeSelector

Imos especificar unha etiqueta no arquivo de definici칩n do pod que non est치 dispo침ible na etiqueta do nodo.

```yaml
# nginx-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx
  nodeSelector:
    ssd: "yes"
```

Cre o pod co manifesto anterior e observe o seu estado:

``` shell
# Crear pod
kubectl create -f nginx-pod.yaml

# Inspeccionar os eventos do pod
kubectl describe pod nginx-pod  | grep -A5 Events
```

Sa칤da:
```
Events:
  Type     Reason            Age    From               Message
  ----     ------            ----   ----               -------
  Warning  FailedScheduling  12m    default-scheduler  0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
```

Na demostraci칩n anterior, podemos ver que *nginx-pod* est치 nun estado pendente debido 치 falla de coincidencia de labels. Por칠n, temos que ter moito cuidado 칩 especificar unha label no arquivo de definici칩n do Pod.

## 九덢잺 nodeAffinity

`nodeAffinity` funciona como `nodeSelector` pero 칠 m치is expresivo e perm칤tenos especificar *soft rules*. Como vimos na fase anterior, se non hai nodos dispo침ibels coa label especificada o pod quedar치 en estado pendente. Utilizando a regra de `nodeAffinity` podemos superar este problema de modo que o planificador despregue o Pod incluso se non pode atopar un nodo coincidente.

Tipos de `nodeAffinity`:

- `requiredDuringSchedulingIgnoredDuringExecution`: *kube-scheduler* non pode programar o Pod a menos que se cumpra a regra.
- `preferredDuringSchedulingIgnoredDuringExecution`: *kube-scheduler* intenta atopar un nodo que cumpra la regla. Se non hai un nodo que cumpra a regra, o planificador segue despregando o Pod.

A frase com칰n entre os dous tipos de `nodeAffinity` 칠 `IgnoredDuringExecution`. Significa que despois de programar un pod nun nodo, se a etiqueta que determinou a selecci칩n do nodo b칩rrase, o Pod executarase tal cal. O pod non se ver치 afectado.

Outra gran vantaxe da `nodeAffinity` 칠 que podemos especificar unha lista de valores cun *operador*. Supo침amos que temos tres tipos de nodos dispo침ibles e cada tipo est치 etiquetado como:
- `size: large`
- `size: medium`
- `size: small` 

Queremos asignar noso pod en nodos grandes e medianos. Nese caso, se utilizamos `nodeSelector`  poderemos especificar s칩 unha *label* cada vez, sexa `size: large` ou `size: medium`. Pero se utilizamos `nodeAffinity` poderemos especificar ambas coma nunha lista de valores e o Pod despregarase nos dous nodos.

### 游댌 requiredDuringSchedulingIgnoredDuringExecution

Imos crear un despregue cunha regra de afinidade de nodo especificada no manifesto:

```yaml
# Exemplo 1 requiredDuringSchedulingIgnoredDuringExecution

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: webserver
  name: webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webserver
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: webserver
    spec:
       affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: size
                 operator: In
                 values:
                 - large
                 - medium
       containers:
        - image: nginx
          name: webserver
```

Podemos utilizar o campo `operator` para especificar un operador l칩xico que Kubernetes utilizar치 para implementar as regras. Pode ser `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt` e `Lt`.

```yaml
# Exemplo 2 requiredDuringSchedulingIgnoredDuringExecution

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: webserver
  name: webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webserver
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: webserver
    spec:
       affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: Size
                 operator: Exists
       containers:
        - image: nginx
          name: webserver
```

### 游댌 preferredDuringSchedulingIgnoredDuringExecution

Na regra `preferredDuringSchedulingIgnoredDuringExecution` podemos especificar un `weight` entre 1 e 100 para cada instancia. Se varios nodos cumpren os requisitos, *kube-scheduler* engade o valor do peso para esa expresi칩n a unha suma. Os nodos co total m치is alto prior칤zanse e os pods progr치manse no nodo que ten a maior prioridade.

Supo침amos que temos dous nodos que cumpren os requisitos: un con `size: large` e outro con `size: medium`. Nese caso *kube-scheduler* tendr치 en conta o peso de cada nodo e engadir치 o peso 치s outras puntuaci칩ns para ese nodo e programar치 o Pod no nodo coa puntuaci칩n final m치is alta.

```yaml
# Exemplo 1 preferredDuringSchedulingIgnoredDuringExecution

apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 15
          preference:
            matchExpressions:
            - key: size
              operator: In
              values:
              - large
        - weight: 20
          preference:
            matchExpressions:
            - key: size
              operator: In
              values:
              - medium
  containers:
  - name: webserver
    image: nginx
```

### 游뚾 Proba preferredDuringSchedulingIgnoredDuringExecution

Despregar un pod con `nodeAffinity` tipo `preferredDuringSchedulingIgnoredDuringExecution`. Engadir especificaci칩ns 치 lista de valores que non est치n dispo침ibles como unha *label* nos nodos.

```yaml
# webserver-pod.yamlapiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 15 # peso 1
          preference:
            matchExpressions:
            - key: key
              operator: In
              values: # valores
              - bar
              - foo
        - weight: 20 # peso 2
          preference:
            matchExpressions:
            - key: key
              operator: In
              values: # valores
              - foo
              - bar
  containers:
  - name: webserver
    image: nginx
```

Agora, despregamos o pod e observamos o estado:

```shell
kubectl create -f webserver-pod.yaml

kubectl get pods webserver
```

Sa칤da:

```shell
--------------------------------------------------
NAME        READY   STATUS    RESTARTS   AGE
webserver   1/1     Running   0          4m4s
```

A diferenza do `nodeSelector`, a칤nda que non haxa ningunha *label* coincidente dispo침칤bel nos nodos, os Pods despr칠ganse igual. 

## 九덢잺 nodeName

`nodeName` 칠 a forma m치is directa de programar un pod no nodo desexado. Se o campo `nodeName` especif칤case na `spec` do Pod, *kube-scheduler* intenta despregar o Pod no nodo indicado.

`nodeName` ten algunhas deficiencias:

- Se o nodo indicado non existe o Pod non se executar치.
- Se o nodo indicado non ten os recursos para acomodar o Pod, o Pod non se executar치.
- Os nomes dos nodos en entornos na nube non sempre son predec칤bels ou est치bels.

A continuaci칩n am칩sase un exemplo de manifesto dun Pod utilizando o campo `nodeName`:

```yaml
# Exemplo de nodeName
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: node01
```







